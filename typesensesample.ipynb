{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamaindex and Typesense\n",
    "\n",
    "### Notes to self on getting the right version of Python installed on my new Win machine as well as setting up VS Code to use the correct version\n",
    "\n",
    "List the installed versions of Python\n",
    "> py --list\n",
    "\n",
    "Install a specific version of Python\n",
    "> winget install Python.Python.3.10\n",
    "\n",
    "Run a specific version of Python\n",
    "> py -3.10\n",
    "\n",
    "List the paths to set VS Code to use the correct Python version\n",
    "> py -0p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/examples/vector_stores/TypesenseDemo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-embeddings-openai\n",
    "!pip install llama-index-vector-stores-typesense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    ")\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import textwrap\n",
    "import os\n",
    "\n",
    "load_dotenv()  # This loads the variables from .env\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data/john_maeda/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.typesense import TypesenseVectorStore\n",
    "from typesense import Client\n",
    "\n",
    "# Connect to Typesense Cloud\n",
    "client = Client({\n",
    "    'api_key': os.getenv('TYPESENSE_API_KEY'),\n",
    "    'nodes': [{\n",
    "        'host': os.getenv('TYPESENSE_HOST'),\n",
    "        'port': os.getenv('TYPESENSE_PORT'),\n",
    "        'protocol': os.getenv('TYPESENSE_PROTOCOL')\n",
    "    }]\n",
    "})\n",
    "collection_name = \"maeda_collection\"\n",
    "typesense_vector_store = TypesenseVectorStore(typesense_client, collection_name=collection_name)\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=typesense_vector_store\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# By default, typesense vector store uses vector search. You need to provide the embedding yourself.\n",
    "query_str = \"What is the relationship between art and technology?\"\n",
    "embed_model = OpenAIEmbedding()\n",
    "# You can also get the settings from the Settings object\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# embed_model = Settings.embed_model\n",
    "query_embedding = embed_model.get_agg_embedding_from_queries(query_str)\n",
    "query_bundle = QueryBundle(query_str, embedding=query_embedding)\n",
    "response = index.as_query_engine().query(query_bundle)\n",
    "\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using text search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "\n",
    "# You can also use text search\n",
    "query_str = \"What is the relationship between art and technology?\"\n",
    "query_str = \"What is the STEAM?\"\n",
    "\n",
    "query_bundle = QueryBundle(query_str=query_str)\n",
    "response = index.as_query_engine(\n",
    "    vector_store_query_mode=VectorStoreQueryMode.TEXT_SEARCH\n",
    ").query(query_bundle)\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = typesense_client.collections[collection_name].delete()\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
