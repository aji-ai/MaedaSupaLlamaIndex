{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Supabase as a vector store with LlamaIndex in Python\n",
    "\n",
    "## Before you get started\n",
    "\n",
    "You will need to have a supabase database with the vector store extension enabled and can follow the llamaindex [docs](https://docs.llamaindex.ai/en/stable/examples/vector_stores/SupabaseVectorIndexDemo/) to get there. Or you can just follow me.\n",
    "\n",
    "![](assets/pgvector.jpg)\n",
    "\n",
    "> Important: For your supabase project's password be sure to use a password that has no special characters or it will break the connection string you'll be using later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "## Install the packages\n",
    "\n",
    "Note that I used Python 3.10.13 and this all worked for me. I tried it in 3.11 and 3.12 and didn't have the same luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-vector-stores-supabase\n",
    "!pip install python-dotenv\n",
    "!pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your `.env` file ready\n",
    "\n",
    "We will use a `.env` to store two pieces of information. Note that it doesn't exist because it's in `.gitignore` so that you don't accidentally commit your secrets. The file should read with two lines like this:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "POSTGRES_CONNECTION_STRING=postgresql:...\n",
    "```\n",
    "\n",
    "The POSTGRES_CONNECTION_STRING is the connection string to your supabase database which has the format `postgresql://<user>:<password>@<host>:<port>/<db_name>` and you can get it by going first to your project settings and then to Database and then to Connection String.\n",
    "\n",
    "![](assets/projsettings.jpg)\n",
    "\n",
    "![](assets/postgres.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the basic packages and key information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, Document, StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.supabase import SupabaseVectorStore\n",
    "from dotenv import load_dotenv\n",
    "import textwrap\n",
    "\n",
    "load_dotenv()  # This loads the variables from .env\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "connection_string = os.getenv(\"POSTGRES_CONNECTION_STRING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to the Supabase vector store\n",
    "\n",
    "### Grab the documents in a directory and prep them for llama index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"maeda_demo\"\n",
    "documents = SimpleDirectoryReader(\"./data/john_maeda/\").load_data()\n",
    "print(f\"> {len(documents)} doc chunks written to vecs schema{collection_name} in supabase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write documents to `collection_name` in supabase\n",
    "\n",
    "> Warning: Be sure to delete the `collection_name` before adding documents to it, or else you'll keep accruing documents into that collection. You can either delete it manually like below, or use the code at the end of this notebook.\n",
    "\n",
    "![](assets/deletecollection.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and write the chunks to supabase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = SupabaseVectorStore(\n",
    "    postgres_connection_string=(\n",
    "       connection_string\n",
    "    ),\n",
    "    collection_name=collection_name,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You're done! Now you can read from the database any time you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from the Supabase vector store\n",
    "\n",
    "### Let's plug llamaindex into the Supabase vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "vector_store = SupabaseVectorStore(\n",
    "    postgres_connection_string=(\n",
    "       connection_string\n",
    "    ),\n",
    "    collection_name=collection_name,\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store = vector_store, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the index however you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# I added this to suppress a warning that stressed me out\n",
    "warnings.filterwarnings(\"ignore\", message=\"Query does not have a covering index for cosine_distance\")\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "query = \"\"\"What is Maeda's george clooney inspiration? \n",
    "  What's the specific takeaway? \n",
    "  Don't make up any detail that isn't in the text.\n",
    "\"\"\"\n",
    "response = query_engine.query(query)\n",
    "print(f\"> {query}\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a lot of fun to talk to oneself ... sorta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"What did the author do growing up? \n",
    "  Say 'I don't know' if you don't know.\n",
    "\"\"\"\n",
    "response = query_engine.query(query)\n",
    "print(f\"> {query}\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the actual chunks related to the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is some utility code for setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core import QueryBundle\n",
    "from IPython.display import Markdown\n",
    "def get_retrieved_nodes(query_str, vector_top_k=5, reranker_top_n=2, with_reranker=False):\n",
    "    query_bundle = QueryBundle(query_str)\n",
    "    # configure retriever\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=index,\n",
    "        similarity_top_k=vector_top_k,\n",
    "    )\n",
    "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "    if with_reranker:\n",
    "        # configure reranker\n",
    "        reranker = LLMRerank(\n",
    "            choice_batch_size=3,\n",
    "            top_n=reranker_top_n,\n",
    "        )\n",
    "        retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
    "\n",
    "    return retrieved_nodes\n",
    "\n",
    "def pretty_print(results):\n",
    "    markdown_str = \"| Score | Text |\\n|-------|------|\\n\"\n",
    "    for result in results:\n",
    "        # Replace line breaks with space for Markdown format\n",
    "        text = result[\"Text\"].replace(\"\\n\", \" \")\n",
    "        markdown_str += \"| {} | {} |\\n\".format(result[\"Score\"], text)\n",
    "    display(Markdown(markdown_str))\n",
    "\n",
    "def visualize_retrieved_nodes(nodes) -> None:\n",
    "    result_dicts = []\n",
    "    for node in nodes:\n",
    "        result_dict = {\"Score\": node.score, \"Text\": node.node.get_text()}\n",
    "        result_dicts.append(result_dict)\n",
    "\n",
    "    pretty_print(result_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Re-ranking it's not working that hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_query = \"What did John Maeda think about the role of technology in art?\"\n",
    "nodes = get_retrieved_nodes(my_new_query)\n",
    "visualize_retrieved_nodes(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Re-ranking it is a bit more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_nodes = get_retrieved_nodes(\n",
    "    my_new_query,\n",
    "    vector_top_k=3,\n",
    "    with_reranker=True,\n",
    ")\n",
    "visualize_retrieved_nodes(new_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a \"tree summarizer\" approach in conjunction with re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[\n",
    "        LLMRerank(\n",
    "            choice_batch_size=5,\n",
    "            top_n=2,\n",
    "        )\n",
    "    ],\n",
    "    verbose=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "response = query_engine.query(\n",
    "    my_new_query,\n",
    ")\n",
    "\n",
    "print(textwrap.fill(str(response), 80))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using citations to get specific details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CitationQueryEngine\n",
    "\n",
    "query_text = \"I would like to hear your thoughts about art and technology. \"\n",
    "\n",
    "response_dict = {}  # Initialize an empty dictionary for response\n",
    "\n",
    "query_engine = CitationQueryEngine.from_args(\n",
    "    index,\n",
    "    similarity_top_k=3,\n",
    "    citation_chunk_size = 1024\n",
    ")\n",
    "myq = query_text # + \" If the facts are not available say, 'I do not know.'\\n\"\n",
    "response = query_engine.query(myq)\n",
    "print(textwrap.fill(f\"Q: {myq}\",80))\n",
    "print(\"\")\n",
    "print(textwrap.fill(f\"ai-johnmaeda response: {response}\",80))\n",
    "print(f\"-- with {len(response.source_nodes)} citations.\")\n",
    "print(\"---\")\n",
    "# response_text = f\"query: {myq} response: {response}\\n\"\n",
    "# # print out all nodes text with the enumeration of node\n",
    "# for i, node in enumerate(response.source_nodes):\n",
    "#     response_text += f\"**{i}**: {node.node.get_text()}\\n\"\n",
    "\n",
    "# Create a dictionary to represent the response\n",
    "response_dict = {\n",
    "    \"text\": f\"Query: {myq}\",\n",
    "    \"citations\": []\n",
    "}\n",
    "\n",
    "# Extract information from each node and add to the response dictionary\n",
    "for i, node in enumerate(response.source_nodes):\n",
    "    node_text = node.node.get_text()\n",
    "    # extract metadata\n",
    "    metadata = node.node.metadata\n",
    "    print(f\"---\\n[{i}]\\n\")\n",
    "    for key, value in metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"\")\n",
    "    # Assuming node_text is a dictionary or can be converted to one\n",
    "    # You may need to adjust this based on the actual structure of node.node.get_text()\n",
    "    print(node_text)\n",
    "    response_dict[\"citations\"].append(node_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You've made it through the notebook!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Supabase vec schema utilities that might help you\n",
    "\n",
    "### List all tables in the vecs schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(connection_string)\n",
    "\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT table_name \n",
    "            FROM information_schema.tables \n",
    "            WHERE table_schema = 'vecs'\n",
    "        \"\"\")\n",
    "        tables = cur.fetchall()\n",
    "        \n",
    "        print(\"Tables in 'vecs' schema:\")\n",
    "        for table in tables:\n",
    "            print(f\"- {table[0]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete a table in the vecs schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(connection_string)\n",
    "conn.autocommit = True\n",
    "\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        drop_table_sql = f'DROP TABLE IF EXISTS vecs.\"{collection_name}\" CASCADE'\n",
    "        cur.execute(drop_table_sql)\n",
    "        print(f\"Table 'vecs.{table_to_delete}' has been deleted successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
